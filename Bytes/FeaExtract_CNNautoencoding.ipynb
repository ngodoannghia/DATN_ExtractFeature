{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from Bytes.cnn_stacking import autoencoder,autoencoder1\n",
    "from Bytes.cnn_stacking import Cnn_Stacking\n",
    "from Bytes.dataloader_csv import CustomDatasetFromImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available(): \n",
    "    print(\"gpu available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint='..'\n",
    "check_name='checkpoint.pth.tar'\n",
    "def save_checkpoint(state, is_best, filename,loss):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new lowest loss : \"+str(loss))\n",
    "        torch.save(state, filename)  # save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_auto(model,_data,num_epochs,learning_rate,w_decay):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_list=[]\n",
    "        for images, labels in _data:\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs,images)\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            l1_reg = None\n",
    "            for W in model.parameters():\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = W.norm(p=1)\n",
    "                else:\n",
    "                    l1_reg = l1_reg + W.norm(p=1)\n",
    "                                     \n",
    "            loss=loss+l1_reg * w_decay\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.item())\n",
    "        print('epoch [{}/{}], mean_loss:{:.4f}'.format(epoch + 1, num_epochs,np.mean(np.array(loss_list))))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_to_auto(model_e,model_t,_data,num_epochs,learning_rate,w_decay):\n",
    "    optimizer = torch.optim.Adam(model_t.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_list=[]\n",
    "        for images, labels in _data:\n",
    "          \n",
    "            images = images.to(device)\n",
    "            \n",
    "            images=model_e.encoder(images)\n",
    "         \n",
    "            outputs = model_t(images)\n",
    "\n",
    "            loss = criterion(outputs,images)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "          \n",
    "            \n",
    "            l1_reg = None\n",
    "            for W in model_t.parameters():\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = W.norm(p=1)\n",
    "                else:\n",
    "                    l1_reg = l1_reg + W.norm(p=1)\n",
    "                                     \n",
    "            loss=loss+l1_reg * w_decay\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.item())\n",
    "        print('epoch [{}/{}], mean_loss:{:.4f}'\n",
    "              .format(epoch + 1, num_epochs,np.mean(np.array(loss_list))\n",
    "        ))\n",
    "    return model_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"../Dataset/data_to_tra.csv\"\n",
    "VAL_DATA_PATH=\"../Dataset/data_to_val.csv\"\n",
    "\n",
    "\n",
    "train_data=CustomDatasetFromImages(TRAIN_DATA_PATH)\n",
    "train_data_loader = data.DataLoader(train_data,batch_size=20,shuffle=True)\n",
    "\n",
    "\n",
    "val_data=CustomDatasetFromImages(VAL_DATA_PATH)\n",
    "val_data_loader = data.DataLoader(val_data,batch_size=20,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs,learning_rate,weight_decay = 100,0.001,1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto=single_auto(autoencoder().to(device).float(),train_data_loader,num_epochs,learning_rate,weight_decay)\n",
    "auto1=auto_to_auto(auto,autoencoder1().to(device).float(),train_data_loader,num_epochs,learning_rate,weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Cnn_Stacking(auto,auto1).to(device).float()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_data_loader)\n",
    "lowest_loss=0.300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_data_loader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs,labels)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_loss_list=[]\n",
    "        for y,(val_images,val_labels) in enumerate(val_data_loader):\n",
    "\n",
    "            val_images=val_images.to(device)\n",
    "\n",
    "            val_labels=val_labels\n",
    "            val_labels=val_labels.to(device)\n",
    "\n",
    "            val_outputs=model(val_images)\n",
    "\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "            val_loss_list.append(val_loss.item())\n",
    "        \n",
    "        mean_loss= np.mean(np.array(val_loss_list))\n",
    "\n",
    "        is_best= bool(mean_loss<lowest_loss)\n",
    "        if(is_best):\n",
    "            lowest_loss= min(mean_loss,lowest_loss)\n",
    "            path= path_to_checkpoint+str(lowest_loss)+\" \"+str(epoch+1)+\" \"+check_name\n",
    "            save_checkpoint({'epoch':epoch + 1,'state_dict': model.state_dict(),'lowest_loss': lowest_loss }, is_best,path,lowest_loss)\n",
    "            is_best= False\n",
    "        \n",
    "        print(\"Epoch [\"+str(epoch+1)+\"/\"+str(num_epochs)+\"],Batch_no[\"+str(i+1)+\"/\"+str(total_step)+\"] \"+\"Loss:\"+str(format(loss.item(),'.4f'))+\" Val_loss:\"+str(format(mean_loss,'.4f')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
